\chapter{Grundlagen}\label{chapter:basics}
In diesem Kapitel sollen die Grundlagen, die in dieser Arbeit benötigt werden,
besprochen werden und bildet den Theorieteil der Thesis. Zuerst wird auf die
mathematischen Grundlagen eingegangen, die zum Definieren von Verlustfunktionen
für generative Machine-Learning-Modelle essentiell sind. Ziel dieses Teils ist
es die Frage zu beantworten, wie weit eine Wahrscheinlichkeitsverteilung von
einer anderen entfernt ist, um die Distanz anschließend zu minimieren. Das
Minimieren der Entfernung zwischen Verteilungen wird in darauf folgenden
Abschnitten verwendet, um Generative-Adversarial-Networks zu trainieren.

Neben den mathematischen Grundlagen werden einige grundlegende Bausteine für die
Entwicklung von hochqualitativen Merkmal-Extraktoren für die Objekterkennung auf
mobilen Endgeräten erläutert. Diese werden in späteren Kapiteln verwendet, um
neue neuronale Netzwerke zu entwickeln, die unter anderem menschliche Posen aus
Bildern sehr genau extrahieren und letztlich Bewegungen erkennen können. Hierfür
ist eine hohe Semantik bei einer hohen Qualität bzw. Auflösung unabdingbar, um
auch kleine Merkmale wie Hände erkennen zu können. Deshalb werden
Feature-Pyramid-Networks (FPNs) erläutert, die mithilfe eines sogenannten
Backbones hochauflösende Merkmale mit einer hohen Semantik extrahieren können,
während einfache Faltungsschichten, auch \textit{Convolutional-Layer} oder kurz
\textit{Conv} genannt, zwar ein hohes
Verständnis entwickeln, jedoch eine kleine Auflösung dieser Merkmale besitzen.

Neben den FPNs sollen auch die Architekturen von Residual-Neural-Networks
(ResNets) und Inverted-Residuals (MobileNetV2) erläutert werden. Diese dienen
meist als Backbones für Merkmal-Extraktoren und sind in Verbindung
mit FPNs essentieller Bestandteil der Posenerkennung.

\section{Notationen}
In dieser Arbeit werden verschiedene Notationen aus der Statistik und dem
Machine-Learning-Umfeld verwendet und sollen hier aufgrund der Lesbarkeit
aufgelistet werden.

\paragraph{Erwartungswert.}
Der Term $\mathbb{E}_{x \sim P}\left[f(x)\right]$ stellt den Erwartungswert
einer Verteilung $P$ dar und liest sich als \textit{erwarteter Wert von
$f(x)$ unter $x$ verteilt als $P$}.

\paragraph{Berechnung von Gradienten.}
Der Term $\nabla_w\left[f(x)\right]$ stellt die Berechnung der Gradienten von
den Parametern $w$ mithilfe der Loss-Funktion $f$ dar.

\paragraph{Euklidische Norm.}
Der Term $\| v \|$ stellt die euklidische Norm von $v$ dar. Sie ist definiert als die quadratische Wurzel der Summe aller Quadrate der Komponenten von $v$, also $\| v \| = \| v \|_2 = \sqrt{v_1^2 + v_2^2 + ... + v_n^2}$.

\section{Lipschitzstetigkeit}
\begin{definition}[K-Lipschitzstetigkeit]
Seien $(X, d_X)$ und $(Y, d_Y)$ metrische Räume. Eine Abbildung $f: X \to Y$
wird als K-lipschitzstetig bezeichnet, wenn
\[
    d_Y(f(x_1), f(x_2)) \leq K \cdot d_X(x_1, x_2)
\]
für alle $x_1, x_2 \in X$ gilt. $K$ wird hierbei als Lipschitzkonstante
bezeichnet und muss immer $K \geq 0$ erfüllen.
\end{definition}

\section{Kullback-Leibler-Divergenz}
Die Kullback-Leibler-Divergenz (KL-Divergenz) misst, wie sehr sich zwei
Verteilungen voneinander unterscheiden und hat seinen Ursprung in der
Informationstheorie. 
\begin{definition}[Kullback-Leibler-Divergenz \cite{arjovsky2017wasserstein}]
Seien $P$ und $Q$ zwei Wahrscheinlichkeitsfunktionen über den gleichen
Wahrscheinlichkeitsraum $X$. Dann ist der Abstand bzw. die Divergenz der
beiden Verteilungen definiert als
\[
    D_{KL}(P \lvert\lvert Q) = \sum_{x \in X} P(x) \log \frac{P(x)}{Q(x)}.
\]
\end{definition}
Dabei gibt $P \lvert\lvert Q$ eine Divergenz von der Ausgangsverteilung $P$
zur Zielverteilung $Q$ an. Das Messen der Divergenz zwischen zwei
Wahrscheinlichkeitsverteilungen findet insbesondere im Machine-Learning statt,
um künstliche neuronale Netze und ihre Gewichte zu trainieren. Deshalb kann
die KL-Divergenz auch als Loss-Funktion verwendet werden. Bemerkenswert ist
hierbei, dass die KL-Divergenz asymmetrisch ist, also $D_{KL}(P \lvert\lvert
Q) \neq D_{KL}(Q \lvert\lvert P)$. Die Distanz zwischen zwei Verteilungen
unterscheidet sich demnach je nach Ausgangsverteilung.

\section{Jensen-Shannon-Divergenz}
\begin{definition}[Jensen-Shannon-Divergenz \cite{arjovsky2017wasserstein}]
Seien $P$ und $Q$ zwei Wahr\-schein\-lichkeitsfunktionen über den gleichen
Wahrscheinlichkeitsraum $X$. Dann ist die Jensen-Shannon-Divergenz der
beiden Verteilungen definiert als
\[
    D_{JS}(P \lvert\lvert Q) = \frac{1}{2} D_{KL}(P \lvert\lvert M) + \frac{1}{2} D_{KL}(Q \lvert\lvert M) \quad\quad \text{mit} \;\; M = \frac{1}{2}(P + Q)
\]
\end{definition}
Die Jensen-Shannon-Divergenz kann als Erweiterung der
Kullback-Leibler-Divergenz angesehen werden. Im Gegensatz zur
Kullback-Leibler-Divergenz ist die Jensen-Shannon-Divergenz (JS-Divergenz)
symmetrisch. Das bedeutet, dass der Abstand zwischen zwei
Wahrscheinlichkeitsverteilungen gleich groß ist, egal von welchen der beiden
Distributionen aus betrachtet wird.

\section{Wasserstein-Abstand}
Eine weitere Methode zum Messen des Abstands zwischen zwei
Wahrscheinlichkeitsverteilungen ist die Berechnung des Wasserstein-Abstands.
Diese Methode wird besonders wichtig für die folgenden Abschnitte, in denen
generative neuronale Netze mithilfe des Wasserstein-Abstandes definiert und
umgesetzt werden.

\begin{definition}[Wasserstein-Abstand \cite{arjovsky2017wasserstein}]
Seien $P_r$ und $P_g$ zwei Wahrscheinlichkeitsverteilungen, dann ist der
Wasserstein-Abstand definiert als
\[
    W(P_r, P_g) = \inf_{\gamma \in \Pi(P_r, P_g)} \mathbb{E}_{(x, y) \sim \gamma} \left[\|x - y\|\right],
\]
wobei $\Pi(P_r, P_g)$ die Menge aller gemeinsamen Verteilungen $\gamma(x,
y)$ darstellt, dessen Grenzen $P_r$ und $P_g$ sind.
\end{definition}

Der Term $\gamma(x, y)$ stellt dabei die \textit{Masse} dar, die von $x$ nach
$y$ transportiert wird, um schließlich die Verteilung $P_r$ in die Verteilung
$P_g$ umzuformen. Aus diesem Grund ist der Wasserstein-Abstand auch als
\textit{Earth-Mover-Abstand} (EM-Abstand) bekannt.

\FloatBarrier
\section{Backbones}
Backbones sind Netzwerke, die vor allem Merkmale (Features) aus der Eingabe, wie
z.B. aus einem Bild, extrahieren, ohne dass eine Abschätzung durchgeführt wird.
Die extrahierten Merkmale werden anschließend verwendet, um beispielsweise eine
Abschätzung mithilfe eines Regressors auszuführen \cite{amjoud2020backbones}.
Man kann demnach verschiedene Backbones mit unterschiedlichen Regressoren
verbinden, ganz nach dem Baukastenprinzip. In dieser Arbeit werden vor allem
ResNet- und MobileNet-Backbones verwendet, da einige Konfigurationen von ihnen
vielversprechend für die Ausführung auf mobilen Geräten sind. Abbildung
\ref{fig:backbone-regressor-framework} soll verdeutlichen, wie
Machine-Learning-Modelle mithilfe eines solchen modularen Frameworks
zusammengestellt werden können.

\begin{figure}
  \centering
  \includegraphics[width=0.9\textwidth]{images/backbone-regressor-framework.pdf}
  \caption{Schematische Darstellung einer Machine-Learning-Architektur, die aus
  einem Backbone für die Merkmal-Extraktion und einem Prediction-Head für die
Klassifizierung bzw. Objektdetektion besteht. Optional können
Feature-Extraktoren wie Feature-Pyramid-Networks o.ä. an das Backbone gehängt
werden, um z.B. die Qualität der Merkmale zu verbessern.}
  \label{fig:backbone-regressor-framework}
\end{figure}

\FloatBarrier

\section{Residual-Neural-Networks}\label{section:residual-blocks}
Mit \cite{he2015deep} wurde ein neues Framework zum Trainieren von tiefen
neuronalen Netzen vorgestellt. Ein großes Problem mit sehr tiefen neuronalen
Netzen ist, dass diese zu einem größeren Fehler im Training führen. Dies führt
ebenfalls zu einem erhöhten Test-Fehler. Dies steht im Konflikt mit der
eigentlichen Vermutung, dass tiefere Netzwerke intelligenter sein müssten, als
weniger tiefe. Diese Fehler werden überraschenderweise immer größer, je tiefer
das Netzwerk ist, was als \textit{Degradation} bezeichnet wird.
Dies ist auch bei der Genauigkeit solcher Netze beobachtbar. Ist die
Genauigkeit gesättigt und erhöht man nun die Tiefe des Netzes, so degradiert
die Genauigkeit. Laut \cite{he2015deep} liegt dies nicht am Overfitting
(Überanpassung). Overfitting beschreibt die Überspezialisierung eines
Machine-Learning-Modells, welches sich zu sehr an den dahinterliegenden
Datensatz angepasst hat. Ein solches Modell kann nicht mehr zuverlässig mit
Daten außerhalb des zum Trainieren verwendeten Datensatzes betrieben werden.
Anders ausgedrückt besitzt das Netzwerk eine sehr hohe Genauigkeit beim
Arbeiten mit dem Trainingsdatensatz, aber eine signifikant niedrigere
Genauigkeit beim Arbeiten mit dem Testdatensatz.

Abhilfe für Degradation sollen die Residual-Networks (ResNets) mithilfe von
\textit{Building-Blocks} schaffen. Anstatt die Abbildung
$F(x)$ bei normalen gestapelten Schichten zu optimieren, wird bei ResNet das
Residuum $F(x) + x$ modelliert. Dies kann mithilfe von Shortcut-Connections
(Abkürzungsverbindungen) realisiert werden (siehe Abbildung
\ref{fig:resnet-building-block}).

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/resnet_building_block.pdf}
    \caption{Schemata für Building-Blocks eines ResNets \cite{he2015deep}. Die
    Shortcut-Connection ist die Identität von dem Eingabeparameter $x$. Links ist ein einfacher Residual-Block dargestellt, während rechts ein tieferer, soganannter Bottleneck-Block dargestellt wird.}
    \label{fig:resnet-building-block}
\end{figure}

Abhängig von den Anforderungen können nun entsprechend tiefe Netze eingesetzt
werden, indem die Building-Blocks verkettet eingesetzt werden. In
\cite{he2015deep} werden außerdem Architekturen vorgestellt, die
erfolgreich auf den ImageNet-Datensatz \cite{deng2009imagenet} evaluiert wurden.
Diese unterscheiden sich hauptsächlich in der Anzahl der verwendeten Schichten
und sind in Tabelle \ref{table:resnets} zu sehen. Jeder dieser Residual-Blöcke,
egal ob einfacher Building- oder Bottleneck-Block, kann einen Stride besitzen. Bottleneck-Blöcke sind tiefere Residual-Blöcke mit drei Faltungsschichten, deren Filter einen Flaschenhals darstellen (vgl. Abbildung \ref{fig:resnet-building-block}).
Dabei ist wichtig, dass der entsprechende Stride nur auf die erste
Faltungsschicht des gesamten Blocks angewandt wird. Die restlichen Schichten
besitzen demnach immer einen Stride von 1. Die Ergebnisse nach dem Trainieren
dieser Netze haben gezeigt, dass das Degradation-Problem gelöst werden konnte,
sodass tiefere Netze mithilfe von Residual-Blocks auch einen geringeren Fehler
erzeugen, also genauer arbeiten als weniger tiefe Netze.

\begin{table}
    \scriptsize
    \begin{tabularx}{\textwidth}{X|X|c|c|c|c|c}
        \hline
        layer name & output size & 18-layer & 34-layer & 50-layer & 101-layer & 152-layer \\ \hline
        conv1 & $112 \times 112$ & \multicolumn{5}{c}{$7 \times 7$, 64, stride 2} \\ \hline
        \multirow{2}{*}{conv2\_x} & \multirow{2}{*}{$56 \times 56$} & \multicolumn{5}{c}{$3 \times 3$, max pool, stride 2} \\ \cline{3-7}
        & & \resnetblocksimple{64}{64}{2} & \resnetblocksimple{64}{64}{3} & \resnetbottleneck{64}{256}{3} & \resnetbottleneck{64}{256}{3} & \resnetbottleneck{64}{256}{3} \\ \hline
        conv3\_x & $28 \times 28$ & \resnetblocksimple{128}{128}{2} & \resnetblocksimple{128}{128}{4} & \resnetbottleneck{128}{512}{4} & \resnetbottleneck{128}{512}{4} & \resnetbottleneck{128}{512}{8} \\ \hline
        conv4\_x & $14 \times 14$ & \resnetblocksimple{256}{256}{2} & \resnetblocksimple{256}{256}{6} & \resnetbottleneck{256}{1024}{6} & \resnetbottleneck{256}{1024}{23} & \resnetbottleneck{256}{1024}{36} \\ \hline
        conv5\_x & $7 \times 7$ & \resnetblocksimple{512}{512}{2} & \resnetblocksimple{512}{512}{3} & \resnetbottleneck{512}{2048}{3} & \resnetbottleneck{512}{2048}{3} & \resnetbottleneck{512}{2048}{3} \\ \hline
        & $1 \times 1$ & \multicolumn{5}{c}{average pool, 1000-d fc, softmax} \\ \hline
        \multicolumn{2}{c|}{FLOPs} & \num{1.8e9} & \num{3.6e9} & \num{3.8e9} & \num{7.6e9} & \num{11.3e9} \\ \hline
    \end{tabularx}
    \caption{Architekturen für ImageNet \cite{he2015deep}. Es werden die gestapelten Schichten bzw. Building-Blocks für ResNet-18, -34, -50, -101 und -152 aufgelistet. Die Building-Blocks werden hier als Matrix dargestellt und geben pro Zeile die Kernelgröße und Anzahl der Kanäle der Faltungsschichten an. Das Multiplikationszeichen hinter den Matrizen gibt die Anzahl der verketteten Building-Blocks an.}
    \label{table:resnets}
\end{table}

\section{MobileNetV2}
Dieser Abschnitt soll nun eine Architektur vorstellen, die geeignet ist, um
Modelle des maschinellen Lernens auf mobilen Geräten auszuführen. Diese soll
als Grundlage zum Verständnis der MoveNet-Architektur aus \cite{movenet}
dienen, die später für die Bewegungserkennung wichtig wird. Die Rede ist
hierbei von MobileNetV2 \cite{sandler2019mobilenetv2} und den damit
eingeführten Inverted-Residual-Blocks. Diese besitzen einen verringerten
Speicherverbrauch bei einer Inferenz, was für mobile Anwendungen sehr wichtig
ist und sind wie folgt aufgebaut. Die Eingabe $x$ in einem solchen Block wird
zuerst in eine Faltungsschicht mit einem $1 \times 1$ Kernel und einer
ReLU6-Aktivierung gegeben. Danach wird eine Depthwise-Separable-Convolution
\cite{howard2017mobilenets} durchgeführt, die eine Kernelgröße von $3 \times 3$
verwendet. Auch hier wird eine ReLU6-Aktivierung vorgenommen. Die nächste
Schicht ist wieder eine $1 \times 1$ Faltungsschicht mit einer linearen
Aktivierungsfunktion. Zum Schluss wird die Ausgabe der letzten Faltungsschicht
mit der Identität der Eingabe $x$ addiert und bildet ein Residuum. Der gesamte
Ablauf ist in Abbildung \ref{fig:inverted-residual} zu sehen und ähnelt sehr
stark den Residual-Blocks aus Abschnitt \ref{section:residual-blocks}. Neben
den üblichen Parametern wurden ein paar wenige hinzugefügt. So beschreibt der
Expansionsfaktor $t$ einen Skalar, der die Anzahl der internen Filter des
Blocks skaliert. Dieser hat keinen Einfluss auf die Eingabe- oder Ausgabegröße.
Das beudetet, dass bei einer Eingabegröße von $h \times w \times c$ die erste
und zweite Faltungsschicht $c \cdot t$ Ausgabefilter besitzen. Auf die dritte
und letzte Schicht eines Inverted-Residual-Blocks hat der Expansionsfaktor
keinen direkten Einfluss. Hier wird lediglich von $c \cdot t$ auf $c'$ Filter
projeziert, indem eine Faltungsschicht mit $c'$ Filtern verwendet wird. Bei dem Stride eines kompletten Blocks verhält es sich ähnlich.
Dieser wird lediglich auf die zweite also
Depthwise-Separable-Convolution-Schicht angewandt. Die restlichen Schichten
besitzen einen Stride von 1.

Das MobileNetV2 ist relativ einfach aufgebaut. Die erste Schicht
ist eine Faltungsschicht gefolgt von 17 Inverted-Residual-Blocks
und einer weiteren Faltungsschicht. Der gesamte Aufbau soll
durch Tabelle \ref{table:mobilenetv2} dargestellt werden. Wie schon eingangs
erwähnt, wird dieses Modell später für die Bewegungs- bzw. Posenerkennung
verwendet. Genauer gesagt wird das MobileNetV2 als Backbone für einen
Merkmal-Extraktor verwendet. Dies soll jedoch in dem nächsten Abschnitt näher
erläutert werden.

\begin{table}
    \begin{tabularx}{\textwidth}{l|l|l|l|l|l}
        \hline
        Input & Layer & t & c & n & s \\
        \hline
        $224 \times 224 \times 3$ & Conv2D, $3 \times 3$ & - & 32 & 1 & 2 \\
        $112 \times 112 \times 32$ & Bottleneck & 1 & 16 & 1 & 1 \\
        $112 \times 112 \times 16$ & Bottleneck & 6 & 24 & 2 & 2 \\
        $56 \times 56 \times 24$ & Bottleneck & 6 & 32 & 3 & 2 \\
        $28 \times 28 \times 32$ & Bottleneck & 6 & 64 & 4 & 2 \\
        $14 \times 14 \times 64$ & Bottleneck & 6 & 96 & 3 & 1 \\
        $14 \times 14 \times 96$ & Bottleneck & 6 & 160 & 3 & 2 \\
        $7 \times 7 \times 160$ & Bottleneck & 6 & 320 & 1 & 1 \\
        $7 \times 7 \times 320$ & Conv2D, $1 \times 1$ & - & 1280 & 1 & 1 \\
        \hline
    \end{tabularx}
    \caption{Aufbau von MobileNetV2 nach \cite{sandler2019mobilenetv2}. Der
    Bottleneck-Block entspricht einem Inverted-Residual-Block (siehe Abbildung
    \ref{fig:inverted-residual}). Der Parameter $t$ beschreibt den
    Skalierungsfaktor, $c$ die Anzahl der Filter, $n$ die Anzahl der aneinander
    geketteten Blöcke und $s$ den Stride eines Block-Verbunds mit $n$ Blöcken.}
    \label{table:mobilenetv2}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{images/inverted_residual.pdf}
    \caption{Darstellung eines Inverted-Residual-Blocks. Der \textit{Dwise}-Block stellt eine Depthwise-Separable-Convolution aus \cite{howard2017mobilenets} dar. Der gesamte Block wird auch als Bottleneck-Block in der MobileNetV2-Architektur bezeichnet. Nicht zu verwechseln mit dem Bottleneck-Block aus dem ResNet-Kontext.}
    \label{fig:inverted-residual}
\end{figure}

\section{Feature-Pyramid-Networks}
Bei der Objekterkennung ist beim Erlernen der Merkmale eines Bildes die
Auflösung dieser Merkmale sehr gering. Feature-Pyramid-Networks (FPN) haben die
Aufgabe, eine hohe Semantik bei einer hohen Auflösung zu generieren. Häufig sind
diese Netzwerke nur Teil eines Backbones bzw. werden dahinter geschaltet. Als
Backbone-Modell kann zum Beispiel AlexNet, MobileNet und ResNet dienen. In
Kombination mit einem FPN werden diese Netze damit zu einem Merkmal-Extractor
umgewandelt, der eine hohe Semantik bei einer hohen Auflösung der Merkmale
erlernen kann. Der Weg von der Eingabe über das Backbone-Modell wird auch als
\textit{Bottom-Up-Pathway} bezeichnet, während der Weg vom Backbone über die
Feature-Pyramid als \textit{Top-Down-Pathway} bezeichnet wird
\cite{lin2017feature}.

Anfänglich wurden FPNs in Verbindung mit ResNet-Backbones eingeführt (siehe
Abbildung \ref{fig:resnet-fpn}). Die Eingabe ist ein $224 \times 224 \times 3$
Bild und wird mithilfe der ersten ResNet-Schicht (C1) in $112 \times 112 \times
64$ Filter mithilfe eines Convolutional-Layers mit einem Stride von 2 und einem
$7 \times 7$ Kernel umgeformt. Anschließend wird die Ausgabe in ein
Max-Pooling-Block gegeben, welcher die Eingabe in $56 \times 56 \times 128$
Filter umwandelt. Die nächsten Blöcke sind für die Einbettung von FPNs am
wichtigsten, denn sie bilden eine Schnittstelle, die verwendet wird, um ein FPN anzubinden. Der folgende Block (C2) besteht aus mehreren Residual-Blocks,
welche zusammen einen Stride von 1 ergeben und 256 Filter erzeugen. Diese Blöcke
werden zusammengefasst auch \textit{Bottlenecks} genannt. Darauf folgen drei
weitere Bottlenecks (C3, C4, C5) jeweils mit einem Stride von 2. Nach C5 ensteht
somit eine Ausgabe von $7 \times 7 \times 2048$. Soweit zum Aufbau des
Bottom-Up-Pathways. Der Top-Down-Pathway wird mithilfe von Verbindungsschichten
(Laterals) mit dem Backbone (C2, C3, C4, C5) verbunden. Diese haben die Aufgabe, die Anzahl der
Filter aus dem Bottom-Up-Pathway anzugleichen, sodass die Ausgaben aus dem Bottom-Up-Pathway mit denen des Top-Down-Pathways addiert werden können. Hierfür werden
Convolutional-Layer mit einem $1 \times 1$ Kernel und 256 Filtern verwendet,
sodass lediglich die Anzahl der Filter transformiert werden. Im Konkreten
bedeutet dies, dass die Ausgabe von C5 von $7 \times 7 \times 2048$ auf die Form
$7 \times 7 \times 256$ gebracht wird.  Die Größe dieser Ausgabe wird nun
mithilfe von Upsampling (M5) verdoppelt und mit der Ausgabe der
Lateralverbindung von C4 addiert (M4). Dies wird mit den übrigen
Lateralverbindungen und Bottleneck-Blöcken verkettet wiederholt, sodass das FPN
schließlich vier Ausgaben mit den Größen $56 \times 56 \times 256$, $28 \times
28 \times 256$, $14 \times 14 \times 256$ und $7 \times 7 \times 256$ (M5, M4,
M3, M2) ausgibt. Betrachtet man nun die letzte Ausgabe M5 relativ zum
Eingabeformat, so besitzt diese Architektur einen Stride von $\frac{224}{56} = 4$.

Kurz zusammengefasst wird eine große Eingabe mithilfe des Bottom-Up-Pathways
bzw. Backbones zu vielen kleinen Filtern umgeformt, um die Merkmale des Bildes
zu verstehen. Der Top-Down-Pathway versucht hingegen diese relativ kleinen Filter hochzuskalieren. Dadurch sollen die hohe Semantik aus dem Bottom-Up-Pathway beibehalten werden und die Filter eine höhere Auflösung erhalten. Das Problem beim
Vergrößern der Filter ist, dass dabei ein Alias-Effekt auftritt, das Bild also
verschwommen wirkt. Hierfür dienen Smoothing-Layer, welche wiederum nichts
weiter als Convolutional-Layer mit einem $3 \times 3$ Kernel und einem Stride
von 1 sind. Entsprechend werden die Ausgaben M5 - M2 verschärft und ergeben
Merkmale mit einer hohen Auflösung und einer hohen Semantik, die nun wahlweise
für die Objekterkennung verwendet werden können.

\begin{figure}
    \includegraphics[width=\textwidth]{images/ResNet_FPN.pdf}
    \caption{Architektur eines Merkmal-Extractors als Feature-Pyramid-Network
    mit ResNet als Backbone.}
    \label{fig:resnet-fpn}
\end{figure}

Ähnlich verhält sich der Vorgang beim Verwenden eines MobileNetV2-Backbones, wie
in Abbildung \ref{fig:mobilenetv2-fpn} dargestellt. Da das MobileNetV2 mehr Blöcke
als das ResNet besitzt, die Anzahl der Pyramidenstufen aber gleich bleiben soll, werden
einige Blöcke übersprungen und sind nicht direkt Teil des Top-Down-Pathways. Im Prinzip
werden lediglich die Blöcke mit Lateralverbindungen mit dem Top-Down-Pathway verknüpft,
welche die letzte Stufe vor der Halbierung der Filtergröße darstellen. Also immer dann,
wenn der Stride 2 ist. Ausnahme ist C5. Hier wird einfach die letzte Schicht des Netzwerks
verwendet. Der Einbau eines FPN erfolgt ansonsten wie vorher erläutert. Die Kombination mit
MobileNetV2 und Feature-Pyramid-Network ist in dieser Arbeit deshalb wichtig, da dieser
Merkmal-Extraktor mit für die Posenerkennung zuständig ist. Besonders, weil die
Merkmale
eine hohe Semantik und eine hohe Auflösung besitzen, können besonders kleine Objekte innerhalb
eines Bildes erkannt und benötigte Informationen extrahiert werden. In Kapitel \ref{chapter:motion-detection}
wird die Posenerkennung genauer erläutert.

\begin{figure}
    \includegraphics[width=\textwidth]{images/MobileNetV2_FPN.pdf}
    \caption{Architektur eines Merkmal-Extractors als Feature-Pyramid-Network
    mit MobileNetV2 als Backbone.}
    \label{fig:mobilenetv2-fpn}
\end{figure}
