\chapter{Entwicklung einer mobilen Anwendung}
Auf Basis der ausgearbeiteten Ergebnisse aus Kapitel
\ref{chapter:motion-detection} soll nun eine mobile Anwendung entwickelt werden.
Ziel ist es die theoretischen Schlussfolgerungen mit einem praktischen
Experiment zu verifizieren. Da sich diese Arbeit vor allem mit dem Problem
beschäftigen soll, wie solche Modelle auf mobilen Plattformen überführt werden
können, soll eine Android-App entwickelt werden, um die Ergebnisse
zusammenzufassend zu präsentieren. Android wird als Plattform gewählt, da es zur
Zeit die am häufigsten vertretene mobile Plattform ist und ein entsprechendes
Gerät leicht zur Verfügung steht. Zum Vergleich, Android besitzt einen
Marktanteil von 72,84\%, iOS einen von 26,34\% und 0,82\% werden von sonstigen
Plattformen
gehalten\footnote{https://www.statista.com/statistics/272698/global-market-share-held-by-mobile-operating-systems-since-2009/
(besucht am 13.08.2021)}.

Die Anforderungen an die App sind recht simpel. Es sollen über die Kamera
Bewegungen identifiziert werden, wobei die erkannte Bewegungsart angezeigt
werden soll. Das Zeitfenster, in welches Bewegungen erkannt werden sollen, wird
auf 60 Frames festgelegt. Bei einer Kamera, die 30 Bilder pro Sekunde aufnehmen
kann, wird also vorausgesetzt, dass die Bewegung innerhalb von zwei Sekunden
eindeutig identifizierbar ist. Zusätzlich muss berücksichtigt werden, dass die
Abtastrate für die Bewegungserkennung entsprechend hoch, also die
Ausführungsdauer der Machine-Learning-Modelle möglichst gering sein soll. Der
Grund dafür ist, dass die Modelle den Flaschenhals der Anwendung darstellen und
eine hohe Abtastrate nur mit entsprechend schnellen Netzwerken möglich ist.

\section{Implementierungsdetails}
Der Einfachheit halber werden zwei KNNs verwendet, um eine Bewegung zu
identifizieren. Das eine Netzwerk hat die Aufgabe, Schlüsselpunkte des
menschlichen Körpers in Bildern zu erkennen. Diese werden anschließend in einen
Puffer mit maximal 60 Elementen zwischengespeichert. Wird ein Element in den
Puffer hinzugefügt, so werden alle anderen Elemente zuerst um eine Position nach
hinten (rechts) verschoben. Der erste Slot ist nun dementsprechend leer und wird
von dem neuen Element belegt. Ist der Puffer bereits voll, so wird das letzte
Element entfernt. Ein Element dieses Puffers sind 17 Schlüsselpunkte des
menschlichen Körpers. Dieser Puffer bildet damit die Eingabe für das zweite
Netzwerk.  Dieses hat die Aufgabe, Schlüsselpunkte aus 60 Kamerabildern einer
Bewegungsklasse zuzuordnen.

Für die Schlüsselpunkterkennung wird MoveNets Lightning-Architektur
\cite{movenet} verwendet. Diese wurde speziell für mobile Geräte entwickelt,
sodass die Erkennung von Schlüs\-sel\-punk\-ten in Echtzeit durchgeführt werden
kann. Hier gilt es zu testen, ob die Performance in der Tat ausreichend ist, um
auf modernen, aber leistungsärmeren Geräten in Echtzeit zu laufen. Für die
Bewegungserkennung werden die verschiedenen Architekturen aus Kapitel
\ref{chapter:motion-detection} verwendet und getestet. Auch hier ist zu
überprüfen, ob diese in Zusammenarbeit mit MoveNet zu einer ausreichenden
Performance kommen. Eine ausreichende Performance wird dann angenommen, wenn
entsprechende Bewegungen über die Kamera bzw. über die entwickelte App richtig
erkannt wurden.

Für die Entwicklung der Android-App wurde \textit{Kotlin} verwendet. Zudem wird
die \textit{Ten\-sor\-Flow-Lite-API} (TFLite) verwendet, um die exportierten
Modelle auf Android auszuführen. Die in dieser Arbeit implementierten Modelle
wurden alle mit der \textit{TensorFlow-API} trainiert.  Da es sich dabei jedoch
um ein Desktop Framework handelt, wurden die trainierten Modelle im
\textit{Saved-Model}-Format gespeichert und anschließend in das TFLite-Format
umgewandelt. 

Es soll nun der allgemeine Aufbau der mobilen Anwendung besprochen werden. Im
ersten Schritt wurde sich Gedanken über mögliche Klassen der Anwendung gemacht.
Diese sind im UML-Klassendiagramm in Abbildung \ref{fig:uml-app} zu sehen. Als
Einstiegspunkt in dieser, sowie in jeder anderen Android-App, dient die
\texttt{MainActivity}-Klasse. Hier werden vor allem die View-Elemente der App
initialisiert. Auch die Modelle für das Erkennen von Bewegungen werden hier
geladen. Dies ist zum einen das MoveNet-Lightning-Modell\footnote{MoveNet-Lightning, https://tfhub.dev/google/movenet/singlepose/lightning/4} für die
Schlüsselpunkterkennung und zum anderen das MotionNet aus Kapitel
\ref{chapter:motion-detection}. Warum die beiden Modelle nicht in ein einziges
Modell zusammengefügt worden sind, hat den Grund, dass die Schlüsselpunkte in
den dazugehörigen Puffer gespeichert werden sollen. Dieser wird dann
anschließend vom MotionNet ausgelesen und interpretiert, sodass eine
Bewegungserkennung stattfinden kann. Die Ausgabe von einem
Schlüsselpunktdetektor kann demnach nicht direkt als Eingabe für das MotionNet
dienen. Dies liegt natürlich auch daran, dass versucht wird, eine Bewegung in
Echtzeit zu erkennen. Deshalb ist es notwendig, dass die Bilder der Kamera auch
in Echtzeit analysiert werden. Erst wenn diese Abtastung ein bestimmtes
Zeitfenster beinhaltet, kann eine zeitabhängige Änderung einer Pose erkannt
werden. Die beiden geladenen Modelle werden nun in eine dafür vorgesehene Klasse
verwaltet. \texttt{MotionDetector} soll die Erkennungslogik implementieren, was
unter anderem die Implementierung des Schlüsselpunktpuffers (siehe Abbildung
\ref{fig:camera-frame-buffer}) bedeutet. Diese Klasse wird abstrakt umgesetzt,
damit neue Modelle zum Erkennen von Bewegungen einfach von ihr erben und
entsprechende Methoden überschreiben können, um den Weg in die Anwendung
leichter finden zu können. Als Beispiel hierfür wurde das \texttt{MotionNet}
implementiert, welches nun von der abstrakten Klasse erbt. Hier werden die
Modelle als \textit{Interpreter} der \textit{TensorFlow-Lite-API} verwaltet. Die
Inferenz der Modelle wird schließlich in den zu überschreibenden Methoden
ausgeführt. Damit die Ausführung der Modelle überhaupt stattfinden kann, müssen
Bilder über die Kamera in die Anwendung geleitet werden. Dies wird mithilfe der
implementierten \texttt{CameraManager}-Klasse umgesetzt. Diese stellt das
aktuelle Bild der Kamera über ein dafür vorgesehenes Steuerelement dar und
bietet eine Schnittstelle zum Abfangen einzelner Frames. Diese Frames werden
anschließend über die \texttt{MainActivity} in die Bewegungserkennung geleitet.
Nach der Inferenz wird das User-Interface mithilfe der \texttt{MainActivity}
aktualisiert und die Ergebnisse dargestellt. Speziell für die Darstellung der
Schlüsselpunkte und der Bewegungsart wird die View-Klasse \texttt{PersonOverlay}
umgesetzt. Diese stellt alle aus der Bewegungserkennung resultierenden
Ergebnisse grafisch dar.

\begin{figure}
    \includegraphics[width=\textwidth]{images/camera_frame_buffer.pdf}
    \caption{Schematische Darstellung des Puffers der Bewegungserkennung,
    welcher $n = 60$ Schlüsselpunkte des menschlichen Körpers speichert.}
    \label{fig:camera-frame-buffer}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{images/app_uml.pdf}
    \caption{UML-Klassendiagramm der mobilen Anwendung zum Testen der
    Machine-Learning-Modelle für die Bewegungserkennung.}
    \label{fig:uml-app}
\end{figure}

\section{Testaufbau und Ergebnisse}
Die Bewegungserkennung aus den vorherigen Abschnitten soll nun mithilfe der
implementierten mobilen Applikation in der Praxis getestet werden. Hierfür muss
vorerst der Testaufbau besprochen werden, sodass die Ergebnisse nachvollziehbar
und reproduzierbar sind. Als Plattform wird Android zusammen mit ausgewählten
mobilen Geräten verwendet. Ziel ist es herauszufinden, ob die Bewegungserkennung
auf modernen, aber auch auf älterer Hardware in Echtzeit ausführbar ist.